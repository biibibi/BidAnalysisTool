#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Wordæ–‡æ¡£å›¾ç‰‡åˆ†ç¦»å·¥å…· - ç®€åŒ–ç‰ˆ
è¾“å…¥: Wordæ–‡æ¡£ (.docx/.doc)
è¾“å‡º: åˆ†ç¦»åçš„Wordæ–‡æ¡£ + å›¾ç‰‡æ–‡ä»¶å¤¹
"""

import os
import sys
from pathlib import Path
import re

try:
    from docx import Document
    from PIL import Image
    from io import BytesIO
    from openai import OpenAI
    from dotenv import load_dotenv
except ImportError as e:
    print(f"ç¼ºå°‘ä¾èµ–åº“: {e}")
    print("è¯·å®‰è£…: pip install python-docx pillow openai")
    sys.exit(1)

# å¯é€‰çš„.docæ–‡ä»¶æ”¯æŒ
try:
    import win32com.client
    DOC_SUPPORT = True
except ImportError:
    DOC_SUPPORT = False
    print("æç¤º: æ— æ³•å¤„ç†.docæ–‡ä»¶ (éœ€è¦: pip install pywin32)")

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(Path(__file__).parent.parent / '.env')

# åˆå§‹åŒ–ç«å±±å¤§æ¨¡å‹å®¢æˆ·ç«¯
def init_ai_client():
    """åˆå§‹åŒ–AIå®¢æˆ·ç«¯"""
    try:
        # è¯»å–ç¯å¢ƒå˜é‡
        ark_api_key = os.getenv('ARK_API_KEY')
        if not ark_api_key:
            print("è­¦å‘Š: æœªè®¾ç½®ARK_API_KEYç¯å¢ƒå˜é‡")
            return None
            
        client = OpenAI(
            base_url="https://ark.cn-beijing.volces.com/api/v3",
            api_key=ark_api_key,
        )
        return client
    except Exception as e:
        print(f"è­¦å‘Š: æ— æ³•åˆå§‹åŒ–AIå®¢æˆ·ç«¯: {e}")
        return None


def generate_image_name(ai_client, context_text, image_index=1, total_images=1):
    """
    ä½¿ç”¨AIç”Ÿæˆå›¾ç‰‡åç§°
    
    Args:
        ai_client: AIå®¢æˆ·ç«¯
        context_text: å›¾ç‰‡å‘¨å›´çš„æ–‡å­—å†…å®¹
        image_index: å½“å‰å›¾ç‰‡åœ¨è¿ç»­å›¾ç‰‡ä¸­çš„ç´¢å¼• 
        total_images: è¿ç»­å›¾ç‰‡æ€»æ•°
        
    Returns:
        str: ç”Ÿæˆçš„å›¾ç‰‡åç§°ï¼ˆä¸å«æ‰©å±•åï¼‰
    """
    if not ai_client:
        return f"å›¾ç‰‡_{image_index:03d}"
    
    try:
        # æ„å»ºæç¤ºè¯ - é’ˆå¯¹èº«ä»½è¯æ˜æ–‡æ¡£ä¼˜åŒ–
        if total_images > 1:
            prompt = f"""
ä½ æ˜¯ä¸“ä¸šçš„æ–‡æ¡£åˆ†æå¸ˆï¼Œéœ€è¦æ ¹æ®ä¸Šä¸‹æ–‡å‡†ç¡®åˆ¤æ–­å›¾ç‰‡å†…å®¹å¹¶ç”Ÿæˆåˆé€‚çš„æ–‡ä»¶åã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context_text[:800]}

åˆ†æè¦æ±‚ï¼š
1. ä»”ç»†é˜…è¯»ä¸Šä¸‹æ–‡ï¼Œé‡ç‚¹å…³æ³¨è·ç¦»å›¾ç‰‡æœ€è¿‘çš„æ ‡é¢˜å’Œæè¿°
2. æ ¹æ®æœ€ç›¸å…³çš„ä¸Šä¸‹æ–‡å†…å®¹åˆ¤æ–­å›¾ç‰‡ç±»å‹
3. ç”Ÿæˆå‡†ç¡®çš„4-8ä¸ªä¸­æ–‡å­—ç¬¦çš„æ–‡ä»¶å
4. è¿™æ˜¯ç¬¬{image_index}å¼ å›¾ç‰‡ï¼Œå…±{total_images}å¼ è¿ç»­å›¾ç‰‡

å¸¸è§å›¾ç‰‡ç±»å‹å‚è€ƒï¼š
- æŠ€æœ¯æ–¹æ¡ˆå›¾ã€ç³»ç»Ÿæ¶æ„å›¾ã€åŠŸèƒ½æ¨¡å—å›¾
- GPSç®¡ç†å›¾ã€ç›‘æ§ç®¡ç†å›¾ã€æ•°æ®ç®¡ç†å›¾
- èº«ä»½è¯ã€æ‰§ç…§ã€è¯ä¹¦ã€æˆæƒä¹¦
- é¡¹ç›®ç»„æˆå‘˜ã€æŠ•æ ‡å•ä½ä¿¡æ¯
- å…¶ä»–ç›¸å…³æ–‡æ¡£å›¾ç‰‡

æ³¨æ„äº‹é¡¹ï¼š
- ä¼˜å…ˆä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­å‡ºç°çš„å…·ä½“åè¯
- å¦‚æœä¸Šä¸‹æ–‡æåˆ°"GPSç®¡ç†"ï¼Œå‘½åä¸º"GPSç®¡ç†"è€Œä¸æ˜¯å…¶ä»–
- å¦‚æœä¸Šä¸‹æ–‡æåˆ°"åŠŸèƒ½åœºæ™¯"ï¼Œå‘½åä¸º"åŠŸèƒ½åœºæ™¯"è€Œä¸æ˜¯å…¶ä»–
- åªè¿”å›æ–‡ä»¶åï¼Œä¸è¦ä»»ä½•è§£é‡Šæˆ–é¢å¤–æ–‡å­—
- é¿å…ä½¿ç”¨ç‰¹æ®Šå­—ç¬¦

è¯·ç›´æ¥è¿”å›æœ€åˆé€‚çš„æ–‡ä»¶åï¼š"""
        else:
            prompt = f"""
ä½ æ˜¯ä¸“ä¸šçš„æ–‡ä»¶å‘½ååŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¸ºå›¾ç‰‡ç”Ÿæˆä¸€ä¸ªå‡†ç¡®çš„ä¸­æ–‡æ–‡ä»¶åã€‚

ä¸Šä¸‹æ–‡å†…å®¹ï¼š
{context_text[:800]}

ç‰¹åˆ«è¯´æ˜ï¼š
- æŠ€æœ¯æ–¹æ¡ˆå›¾ã€ç³»ç»Ÿæ¶æ„å›¾ã€åŠŸèƒ½æ¨¡å—å›¾
- GPSç®¡ç†å›¾ã€ç›‘æ§ç®¡ç†å›¾ã€æ•°æ®ç®¡ç†å›¾
- èº«ä»½è¯ã€æ‰§ç…§ã€è¯ä¹¦ã€æˆæƒä¹¦
- é¡¹ç›®ç»„æˆå‘˜ã€æŠ•æ ‡å•ä½ä¿¡æ¯

è¦æ±‚ï¼š
1. æ ¹æ®ä¸Šä¸‹æ–‡åˆ¤æ–­å›¾ç‰‡ç±»å‹ï¼Œä¼˜å…ˆä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„å…³é”®è¯
2. ä½¿ç”¨4-8ä¸ªä¸­æ–‡å­—ç¬¦  
3. åªè¿”å›æ–‡ä»¶åï¼Œä¸è¦å…¶ä»–è§£é‡Š
4. é¿å…ä½¿ç”¨ç‰¹æ®Šå­—ç¬¦å’Œç©ºæ ¼

ç¤ºä¾‹ï¼šåŠŸèƒ½åœºæ™¯å»ºè®¾ã€GPSç®¡ç†ç³»ç»Ÿã€æ³•å®šä»£è¡¨äººèº«ä»½è¯
"""

        # è·å–è±†åŒ…æ¨¡å‹ID
        model_id = os.getenv('DOUBAO_MODEL_ID', 'doubao-seed-1-6-250615')
        
        # è°ƒè¯•æ—¥å¿—
        if os.getenv('AI_NAME_LOG'):
            print(f"ğŸ¤– ä½¿ç”¨è±†åŒ…æ¨¡å‹: {model_id}")
            print(f"ğŸ“ ä¸Šä¸‹æ–‡é•¿åº¦: {len(context_text)}")
            print(f"ğŸ“„ ä¸Šä¸‹æ–‡å†…å®¹: {context_text[:100]}...")
        
        completion = ai_client.chat.completions.create(
            model=model_id,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡ä»¶å‘½ååŠ©æ‰‹ï¼Œæ“…é•¿æ ¹æ®æ–‡æ¡£å†…å®¹ç”Ÿæˆåˆé€‚çš„å›¾ç‰‡æ–‡ä»¶åã€‚"},
                {"role": "user", "content": prompt},
            ],
            temperature=0.3,
            max_tokens=50,
            timeout=10  # æ·»åŠ 10ç§’è¶…æ—¶
        )
        
        generated_name = completion.choices[0].message.content.strip()
        
        # è°ƒè¯•æ—¥å¿—
        if os.getenv('AI_NAME_LOG'):
            print(f"ğŸ¯ AIç”Ÿæˆåç§°: {generated_name}")
        
        # æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦
        cleaned_name = re.sub(r'[^\w\u4e00-\u9fff]', '', generated_name)
        
        # å¦‚æœæ˜¯è¿ç»­å›¾ç‰‡ä¸”ç”Ÿæˆçš„åç§°æ²¡æœ‰æ•°å­—åç¼€ï¼Œæ·»åŠ åç¼€
        if total_images > 1 and not re.search(r'\d+$', cleaned_name):
            cleaned_name = f"{cleaned_name}_{image_index}"
            
        return cleaned_name if cleaned_name else f"å›¾ç‰‡_{image_index:03d}"
        
    except Exception as e:
        print(f"AIå‘½åå¤±è´¥: {e}")
        # ç®€å•åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆæè¿°æ€§åç§°
        if context_text:
            # æå–å…³é”®è¯
            words = context_text.strip()[:50]
            if "è®¤è¯" in words:
                return f"è®¤è¯_{image_index:03d}"
            elif "è¯ä¹¦" in words:
                return f"è¯ä¹¦_{image_index:03d}"
            elif "æµ‹è¯•" in words or "æ£€æµ‹" in words:
                return f"æµ‹è¯•_{image_index:03d}"
            elif "æ‰§ç…§" in words or "è¥ä¸š" in words:
                return f"æ‰§ç…§_{image_index:03d}"
            elif "æˆæƒ" in words:
                return f"æˆæƒ_{image_index:03d}"
        
        return f"å›¾ç‰‡_{image_index:03d}"


def is_empty_or_whitespace_paragraph(paragraph):
    """åˆ¤æ–­æ®µè½æ˜¯å¦ä¸ºç©ºæˆ–åªåŒ…å«ç©ºç™½ç¬¦"""
    text = paragraph.text.strip()
    return len(text) == 0


def collect_headings(doc):
    """æ”¶é›†æ–‡æ¡£ä¸­çš„æ ‡é¢˜ä¿¡æ¯"""
    headings = []
    for i, para in enumerate(doc.paragraphs):
        if para.style and ('Heading' in para.style.name or 'heading' in para.style.name.lower()):
            text = para.text.strip()
            if text:
                headings.append({
                    'index': i,
                    'text': text,
                    'style': para.style.name,
                    'level': getattr(para.style, 'level', 1) or 1
                })
    return headings


def find_section_for_image(headings, image_para_idx, total_paras):
    """æ‰¾åˆ°å›¾ç‰‡æ‰€å±çš„ç« èŠ‚"""
    current_section = None
    next_section_start = total_paras
    
    for i, heading in enumerate(headings):
        if heading['index'] < image_para_idx:
            current_section = heading
        elif heading['index'] > image_para_idx:
            next_section_start = heading['index']
            break
    
    if current_section:
        section_start = current_section['index']
        section_end = next_section_start
        return current_section, section_start, section_end
    
    return None, 0, total_paras


def extract_smart_name_from_contexts(contexts):
    """
    ä»ä¸Šä¸‹æ–‡åˆ—è¡¨ä¸­æ™ºèƒ½æå–å›¾ç‰‡åç§°
    
    Args:
        contexts: ä¸Šä¸‹æ–‡æ®µè½åˆ—è¡¨
        
    Returns:
        str: æå–çš„å›¾ç‰‡åç§°ï¼Œå¦‚æœæ— æ³•æå–åˆ™è¿”å›None
    """
    if not contexts:
        return None
        
    # å®šä¹‰ä¸åŒç±»å‹çš„å…³é”®è¯æ¨¡å¼å’Œå¯¹åº”çš„åç§°
    naming_patterns = [
        # è¯ä»¶ç±»
        {
            'keywords': ['è¥ä¸šæ‰§ç…§', 'æ‰§ç…§'],
            'name': 'è¥ä¸šæ‰§ç…§'
        },
        {
            'keywords': ['èº«ä»½è¯'],
            'name': 'èº«ä»½è¯'
        },
        {
            'keywords': ['æ³•å®šä»£è¡¨äºº', 'æ³•äººä»£è¡¨'],
            'name': 'æ³•å®šä»£è¡¨äººèº«ä»½è¯'
        },
        {
            'keywords': ['æˆæƒå§”æ‰˜ä¹¦', 'å§”æ‰˜ä¹¦'],
            'name': 'æˆæƒå§”æ‰˜ä¹¦'
        },
        {
            'keywords': ['æˆæƒä»£ç†äºº', 'ä»£ç†äºº'],
            'name': 'æˆæƒä»£ç†äººèº«ä»½è¯'
        },
        
        # æŠ€æœ¯æ–¹æ¡ˆç±»
        {
            'keywords': ['åŠŸèƒ½åœºæ™¯å»ºè®¾', 'åŠŸèƒ½åœºæ™¯'],
            'name': 'åŠŸèƒ½åœºæ™¯å»ºè®¾'
        },
        {
            'keywords': ['æŠ€æœ¯æ¶æ„è®¾è®¡', 'æŠ€æœ¯æ¶æ„'],
            'name': 'æŠ€æœ¯æ¶æ„è®¾è®¡'
        },
        {
            'keywords': ['æ€»ä½“æ¶æ„è®¾è®¡', 'æ€»ä½“æ¶æ„'],
            'name': 'æ€»ä½“æ¶æ„è®¾è®¡'
        },
        {
            'keywords': ['ç³»ç»Ÿæ¶æ„', 'æ¶æ„è®¾è®¡'],
            'name': 'ç³»ç»Ÿæ¶æ„'
        },
        
        # GPSå’Œç®¡ç†ç±»
        {
            'keywords': ['GPSç®¡ç†', 'GPSç³»ç»Ÿ'],
            'name': 'GPSç®¡ç†'
        },
        {
            'keywords': ['é¡¹ç›®GPSç®¡ç†'],
            'name': 'é¡¹ç›®GPSç®¡ç†'
        },
        {
            'keywords': ['è½¦è¾†GPSæ˜ç»†', 'è½¦è¾†GPS'],
            'name': 'è½¦è¾†GPSæ˜ç»†'
        },
        {
            'keywords': ['é¡¹ç›®è®¾å¤‡ç®¡ç†', 'è®¾å¤‡ç®¡ç†'],
            'name': 'é¡¹ç›®è®¾å¤‡ç®¡ç†'
        },
        {
            'keywords': ['è§†é¢‘ç›‘æ§ç³»ç»Ÿç›‘æ§ç®¡ç†', 'è§†é¢‘ç›‘æ§ç®¡ç†', 'ç›‘æ§ç®¡ç†'],
            'name': 'ç›‘æ§ç®¡ç†'
        },
        {
            'keywords': ['è½¨è¿¹ç‚¹ä½ç®¡ç†', 'ç‚¹ä½ç®¡ç†'],
            'name': 'ç‚¹ä½ç®¡ç†'
        },
        {
            'keywords': ['æ–°å¢ç‚¹ä½'],
            'name': 'æ–°å¢ç‚¹ä½'
        },
        {
            'keywords': ['å›æ”¾å·¥ä½œå°', 'å›æ”¾'],
            'name': 'å›æ”¾å·¥ä½œå°'
        },
        {
            'keywords': ['æ™ºèƒ½ç­›é€‰'],
            'name': 'æ™ºèƒ½ç­›é€‰'
        },
        {
            'keywords': ['å‘Šè­¦ç®¡ç†'],
            'name': 'å‘Šè­¦ç®¡ç†'
        },
        {
            'keywords': ['GPSå¼‚å¸¸'],
            'name': 'GPSå¼‚å¸¸'
        },
        {
            'keywords': ['GPSè½¨è¿¹åç§»å¼‚å¸¸', 'è½¨è¿¹åç§»'],
            'name': 'GPSè½¨è¿¹åç§»'
        },
        {
            'keywords': ['è½¦è¾†å¼‚å¸¸åœç•™'],
            'name': 'è½¦è¾†å¼‚å¸¸åœç•™'
        },
        {
            'keywords': ['éå·¥ä½œæ—¶é—´å¼‚å¸¸'],
            'name': 'éå·¥ä½œæ—¶é—´å¼‚å¸¸'
        },
        {
            'keywords': ['å·¡æŸ¥ç®¡ç†'],
            'name': 'å·¡æŸ¥ç®¡ç†'
        },
        {
            'keywords': ['ç»¼åˆé©¾é©¶èˆ±'],
            'name': 'ç»¼åˆé©¾é©¶èˆ±'
        },
        
        # æŠ•æ ‡ç›¸å…³
        {
            'keywords': ['æŠ•æ ‡å•ä½å®åŠ›', 'å•ä½å®åŠ›'],
            'name': 'æŠ•æ ‡å•ä½å®åŠ›'
        },
        {
            'keywords': ['é¡¹ç›®ç»„æˆå‘˜'],
            'name': 'é¡¹ç›®ç»„æˆå‘˜'
        },
        {
            'keywords': ['æŠ€æœ¯å’Œå•†åŠ¡ç¬¦åˆæ€§', 'ç¬¦åˆæ€§'],
            'name': 'æŠ€æœ¯å•†åŠ¡ç¬¦åˆæ€§'
        }
    ]
    
    # æ£€æŸ¥æ¯ä¸ªä¸Šä¸‹æ–‡æ®µè½ï¼Œæ‰¾åˆ°æœ€åŒ¹é…çš„æ¨¡å¼
    for context in contexts:
        text = context['text']
        
        # æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥æ¨¡å¼ï¼ˆä¼˜å…ˆåŒ¹é…æ›´å…·ä½“çš„å…³é”®è¯ï¼‰
        for pattern in naming_patterns:
            for keyword in pattern['keywords']:
                if keyword in text:
                    return pattern['name']
    
    # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°é¢„å®šä¹‰æ¨¡å¼ï¼Œå°è¯•ä»æ ‡é¢˜ä¸­æå–
    for context in contexts:
        if context.get('is_heading') and context['text']:
            title = context['text'].strip()
            # æ¸…ç†æ ‡é¢˜ï¼Œä¿ç•™ä¸»è¦éƒ¨åˆ†
            clean_title = re.sub(r'[^\w\u4e00-\u9fff]', '', title)
            if 4 <= len(clean_title) <= 12:
                return clean_title
    
    return None


def find_nearest_context_paragraphs(doc, image_para_idx, max_distance=20):
    """
    ä¸ºå›¾ç‰‡å¯»æ‰¾æœ€è¿‘çš„5æ®µæœ‰æ•ˆä¸Šä¸‹æ–‡æ–‡å­—
    
    Args:
        doc: Wordæ–‡æ¡£å¯¹è±¡
        image_para_idx: å›¾ç‰‡æ‰€åœ¨æ®µè½ç´¢å¼•
        max_distance: æœ€å¤§æœç´¢è·ç¦»
        
    Returns:
        list: æœ€æœ‰æ•ˆçš„5æ®µä¸Šä¸‹æ–‡æ®µè½ä¿¡æ¯
    """
    context_candidates = []
    
    # å‘å‰å’Œå‘åæœç´¢æœ‰æ„ä¹‰çš„å†…å®¹
    search_range = range(
        max(0, image_para_idx - max_distance), 
        min(len(doc.paragraphs), image_para_idx + max_distance + 1)
    )
    
    for para_idx in search_range:
        if para_idx >= len(doc.paragraphs) or para_idx == image_para_idx:
            continue
            
        para = doc.paragraphs[para_idx]
        text = para.text.strip()
        
        if not text:
            continue
            
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜
        is_heading = para.style and ('Heading' in para.style.name or 'heading' in para.style.name.lower())
        
        # è¿‡æ»¤æ‰å®Œå…¨æ— æ„ä¹‰çš„å†…å®¹
        ignore_patterns = [
            '[å›¾ç‰‡:', 'å›¾ç‰‡è¯´æ˜', 'å¦‚ä¸‹å›¾', 'è§ä¸‹å›¾', 'ä¸Šå›¾', 'ä¸‹å›¾',
            'å›¾æ‰€ç¤º', 'å¦‚å›¾', 'æ’å›¾', 'é™„å›¾'
        ]
        
        # è¶…é«˜ä»·å€¼çš„æè¿°æ€§å†…å®¹ï¼ˆè¯ä»¶ç±»ï¼‰
        super_high_value_patterns = [
            'èº«ä»½è¯', 'è¥ä¸šæ‰§ç…§', 'æ³•å®šä»£è¡¨äºº', 'æˆæƒä»£ç†äºº', 'æˆæƒå§”æ‰˜äºº',
            'å•ä½è´Ÿè´£äºº', 'è‡ªç„¶äºº', 'ç»„ç»‡æœºæ„ä»£ç è¯', 'ç¨åŠ¡ç™»è®°è¯',
            'å¼€æˆ·è®¸å¯è¯', 'é“¶è¡Œè´¦æˆ·', 'èµ„è´¨è¯ä¹¦'
        ]
        
        # é«˜ä»·å€¼çš„æè¿°æ€§å†…å®¹
        high_value_patterns = [
            'æ‰«æä»¶', 'å¤å°ä»¶', 'æ­£æœ¬', 'å‰¯æœ¬', 'åŸä»¶', 'é™„ä»¶',
            'è¯ä¹¦', 'è®¤è¯', 'è®¸å¯è¯', 'è¯æ˜', 'å§”æ‰˜ä¹¦',
            'èµ„è´¨', 'æ‰§ç…§', 'ç™»è®°', 'å¤‡æ¡ˆ', 'å®¡æ‰¹'
        ]
        
        # ä¸­ç­‰ä»·å€¼çš„å†…å®¹
        medium_value_patterns = [
            'å…¬å¸', 'ä¼ä¸š', 'å•ä½', 'å§“å', 'åç§°', 'åœ°å€', 
            'ç”µè¯', 'è”ç³»', 'èŒåŠ¡', 'éƒ¨é—¨', 'è¯´æ˜', 'ææ–™',
            'æŠ•æ ‡äºº', 'ä¾›åº”å•†', 'æ‰¿åŒ…å•†', 'ç”²æ–¹', 'ä¹™æ–¹'
        ]
        
        # ä½ä»·å€¼ä½†ä»æœ‰ç”¨çš„å†…å®¹
        low_value_patterns = [
            'é¡¹ç›®', 'å·¥ç¨‹', 'é‡‡è´­', 'æ‹›æ ‡', 'æŠ•æ ‡', 'åˆåŒ',
            'æœåŠ¡', 'äº§å“', 'æŠ€æœ¯', 'æ–¹æ¡ˆ', 'è¦æ±‚'
        ]
        
        should_ignore = False
        for pattern in ignore_patterns:
            if pattern in text:
                should_ignore = True
                break
                
        if should_ignore:
            continue
        
        # è®¡ç®—æ®µè½ä»·å€¼åˆ†æ•°ï¼ˆæ›´ç²¾ç¡®çš„è¯„ä¼°ï¼‰
        distance = abs(para_idx - image_para_idx)
        value_score = 0
        
        # åŸºç¡€åˆ†æ•°ï¼šè·ç¦»è¶Šè¿‘åˆ†æ•°è¶Šé«˜ï¼ˆéçº¿æ€§è¡°å‡ï¼‰
        if distance <= 3:
            distance_score = 30 - (distance * 5)  # è·ç¦»1-3æ®µ: 25-15åˆ†
        elif distance <= 8:
            distance_score = 15 - (distance - 3) * 2  # è·ç¦»4-8æ®µ: 13-3åˆ†
        else:
            distance_score = max(1, 10 - distance)  # è·ç¦»9+æ®µ: é€’å‡åˆ°1åˆ†
        
        value_score += distance_score
        
        # æ ‡é¢˜ç‰¹åˆ«åŠ åˆ†
        if is_heading:
            value_score += 40
            
        # å†…å®¹ä»·å€¼åŠ åˆ†ï¼ˆåˆ†å±‚è¯„ä¼°ï¼‰
        content_bonus = 0
        for pattern in super_high_value_patterns:
            if pattern in text:
                content_bonus = max(content_bonus, 50)  # è¶…é«˜ä»·å€¼
                
        if content_bonus == 0:  # å¦‚æœæ²¡æœ‰è¶…é«˜ä»·å€¼ï¼Œæ£€æŸ¥é«˜ä»·å€¼
            for pattern in high_value_patterns:
                if pattern in text:
                    content_bonus = max(content_bonus, 30)  # é«˜ä»·å€¼
                    
        if content_bonus == 0:  # å¦‚æœæ²¡æœ‰é«˜ä»·å€¼ï¼Œæ£€æŸ¥ä¸­ç­‰ä»·å€¼
            for pattern in medium_value_patterns:
                if pattern in text:
                    content_bonus = max(content_bonus, 15)  # ä¸­ç­‰ä»·å€¼
                    
        if content_bonus == 0:  # å¦‚æœæ²¡æœ‰ä¸­ç­‰ä»·å€¼ï¼Œæ£€æŸ¥ä½ä»·å€¼
            for pattern in low_value_patterns:
                if pattern in text:
                    content_bonus = max(content_bonus, 5)  # ä½ä»·å€¼
        
        value_score += content_bonus
        
        # é•¿åº¦è¯„ä¼°ï¼ˆä¿¡æ¯å¯†åº¦ï¼‰
        text_length = len(text)
        if 10 <= text_length <= 50:  # æœ€ä½³é•¿åº¦
            value_score += 10
        elif 5 <= text_length <= 100:  # è‰¯å¥½é•¿åº¦
            value_score += 5
        elif text_length > 200:  # å¤ªé•¿æ‰£åˆ†
            value_score -= 5
            
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®ä¿¡æ¯
        has_key_info = any(keyword in text for keyword in super_high_value_patterns + high_value_patterns)
        
        # ä½ç½®åå¥½ï¼ˆå›¾ç‰‡å‰åçš„å†…å®¹å¯èƒ½æ›´ç›¸å…³ï¼‰
        if para_idx < image_para_idx:  # å›¾ç‰‡å‰çš„å†…å®¹
            position_bonus = 3
        else:  # å›¾ç‰‡åçš„å†…å®¹
            position_bonus = 2
        value_score += position_bonus
        
        context_candidates.append({
            'para_idx': para_idx,
            'text': text,
            'distance': distance,
            'value_score': value_score,
            'is_heading': is_heading,
            'has_key_info': has_key_info,
            'text_length': text_length,
            'content_bonus': content_bonus,
            'distance_score': distance_score
        })
    
    # æŒ‰ä»·å€¼åˆ†æ•°æ’åºï¼Œé€‰æ‹©æœ€å¥½çš„5ä¸ª
    context_candidates.sort(key=lambda x: x['value_score'], reverse=True)
    
    # è¿”å›æœ€æœ‰æ•ˆçš„5ä¸ªæ®µè½
    return context_candidates[:5]


def find_nearby_meaningful_content(doc, image_para_idx, max_distance=10):
    """
    æŸ¥æ‰¾å›¾ç‰‡é™„è¿‘çš„æœ‰æ„ä¹‰æ ‡é¢˜æˆ–æ®µè½å†…å®¹
    
    Args:
        doc: Wordæ–‡æ¡£å¯¹è±¡
        image_para_idx: å›¾ç‰‡æ‰€åœ¨æ®µè½ç´¢å¼•
        max_distance: æœ€å¤§æœç´¢è·ç¦»
        
    Returns:
        tuple: (content_text, content_type, context_paragraphs) å†…å®¹æ–‡æœ¬ã€ç±»å‹å’Œä¸Šä¸‹æ–‡æ®µè½
    """
    meaningful_content = []
    context_paragraphs = []
    
    # å‘å‰å’Œå‘åæœç´¢æœ‰æ„ä¹‰çš„å†…å®¹
    search_range = range(
        max(0, image_para_idx - max_distance), 
        min(len(doc.paragraphs), image_para_idx + max_distance + 1)
    )
    
    for para_idx in search_range:
        if para_idx >= len(doc.paragraphs):
            continue
            
        para = doc.paragraphs[para_idx]
        text = para.text.strip()
        
        if not text:
            continue
            
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜
        is_heading = para.style and ('Heading' in para.style.name or 'heading' in para.style.name.lower())
        
        # è¿‡æ»¤æ‰æ— æ„ä¹‰çš„å†…å®¹ï¼Œä½†ä¿ç•™æè¿°æ€§å†…å®¹
        ignore_patterns = [
            '[å›¾ç‰‡:', 'ç­¾å', 'ç›–ç« ', 'ç”µå­ç­¾å'
        ]
        
        # ç‰¹åˆ«å…³æ³¨çš„æè¿°æ€§å†…å®¹
        descriptive_patterns = [
            'æ‰«æä»¶', 'å¤å°ä»¶', 'æ­£æœ¬', 'å‰¯æœ¬', 'åŸä»¶', 'é™„ä»¶',
            'èº«ä»½è¯', 'è¥ä¸šæ‰§ç…§', 'æ‰§ç…§', 'è¯ä¹¦', 'è®¤è¯', 
            'æˆæƒä»£ç†äºº', 'æ³•å®šä»£è¡¨äºº', 'å•ä½è´Ÿè´£äºº',
            'èµ„è´¨', 'è®¸å¯è¯', 'è¯æ˜', 'å§”æ‰˜ä¹¦'
        ]
        
        is_meaningful = True
        is_descriptive = False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æè¿°æ€§å†…å®¹
        for pattern in descriptive_patterns:
            if pattern in text:
                is_descriptive = True
                break
                
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥å¿½ç•¥
        for pattern in ignore_patterns:
            if pattern in text:
                is_meaningful = False
                break
        
        # å¦‚æœæ–‡å­—å¤ªçŸ­ä½†åŒ…å«æè¿°æ€§å†…å®¹ï¼Œä¹Ÿè®¤ä¸ºæ˜¯æœ‰æ„ä¹‰çš„
        if is_meaningful and (len(text) > 3 or is_descriptive):
            distance = abs(para_idx - image_para_idx)
            
            # è®¡ç®—ä¼˜å…ˆçº§ï¼šæ ‡é¢˜ > æè¿°æ€§å†…å®¹ > æ™®é€šå†…å®¹
            if is_heading:
                priority = 1000
            elif is_descriptive:
                priority = 500  # æè¿°æ€§å†…å®¹ä¼˜å…ˆçº§è¾ƒé«˜
            else:
                priority = 100
                
            priority -= distance  # è·ç¦»è¶Šè¿‘ä¼˜å…ˆçº§è¶Šé«˜
            
            # æå–å…³é”®è¯
            key_words = []
            content_keywords = [
                'èº«ä»½è¯', 'è¥ä¸šæ‰§ç…§', 'æ‰§ç…§', 'è¯ä¹¦', 'è®¤è¯', 
                'æˆæƒä»£ç†äºº', 'æ³•å®šä»£è¡¨äºº', 'å•ä½è´Ÿè´£äºº',
                'èµ„è´¨', 'è®¸å¯è¯', 'è¯æ˜', 'å§”æ‰˜ä¹¦'
            ]
            
            for keyword in content_keywords:
                if keyword in text:
                    key_words.append(keyword)
            
            content_item = {
                'text': text,
                'para_idx': para_idx,
                'distance': distance,
                'is_heading': is_heading,
                'is_descriptive': is_descriptive,
                'priority': priority,
                'keywords': key_words
            }
            
            meaningful_content.append(content_item)
            
            # æ”¶é›†æœ‰æ„ä¹‰çš„æ®µè½ä½œä¸ºä¸Šä¸‹æ–‡ï¼ˆæœ€å¤š3ä¸ªï¼‰
            if len(context_paragraphs) < 3:
                context_paragraphs.append(content_item)
    
    if not meaningful_content:
        return None, None, []
    
    # æŒ‰ä¼˜å…ˆçº§æ’åºï¼Œé€‰æ‹©æœ€ä½³å†…å®¹
    meaningful_content.sort(key=lambda x: x['priority'], reverse=True)
    best_content = meaningful_content[0]
    
    # æŒ‰è·ç¦»æ’åºé€‰æ‹©æœ€è¿‘çš„3ä¸ªæ®µè½ä½œä¸ºä¸Šä¸‹æ–‡
    context_paragraphs = sorted(meaningful_content, key=lambda x: x['distance'])[:3]
    
    # ç”Ÿæˆç®€æ´çš„æè¿°åç§°
    text = best_content['text']
    keywords = best_content['keywords']
    
    if keywords:
        # å¦‚æœæœ‰å…³é”®è¯ï¼Œä½¿ç”¨å…³é”®è¯
        name = keywords[0]
    else:
        # å¦åˆ™ä»æ–‡æœ¬ä¸­æå–å…³é”®éƒ¨åˆ†
        # æ¸…ç†æ–‡æœ¬ï¼Œå»é™¤ç‰¹æ®Šå­—ç¬¦
        clean_text = re.sub(r'[^\w\u4e00-\u9fff\s]', '', text)
        words = clean_text.split()
        
        # é€‰æ‹©æœ€æœ‰æ„ä¹‰çš„è¯æ±‡
        if len(words) > 0:
            # å–å‰å‡ ä¸ªè¯æˆ–è€…é™åˆ¶é•¿åº¦
            name = ''.join(words[:2]) if len(words) > 1 else words[0]
            if len(name) > 8:
                name = name[:8]
        else:
            name = 'æ–‡æ¡£å†…å®¹'
    
    content_type = 'æ ‡é¢˜' if best_content['is_heading'] else ('æè¿°' if best_content['is_descriptive'] else 'æ®µè½')
    return name, content_type, context_paragraphs


def check_section_content(doc, section_start, section_end, image_para_idx):
    """æ£€æŸ¥ç« èŠ‚å†…å®¹æ˜¯å¦åªæœ‰å›¾ç‰‡æ²¡æœ‰æœ‰æ„ä¹‰çš„æ–‡å­—"""
    meaningful_text = []
    image_count = 0
    
    for para_idx in range(section_start + 1, section_end):  # è·³è¿‡æ ‡é¢˜æœ¬èº«
        if para_idx >= len(doc.paragraphs):
            break
            
        para = doc.paragraphs[para_idx]
        text = para.text.strip()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡
        has_image = False
        for run in para.runs:
            if run._element.xpath('.//a:blip'):  # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡
                has_image = True
                image_count += 1
                break
        
        # å¦‚æœä¸æ˜¯å›¾ç‰‡æ®µè½ä¸”æœ‰æœ‰æ„ä¹‰çš„æ–‡å­—ï¼ˆæ’é™¤å¸¸è§çš„æ— æ„ä¹‰æ–‡å­—ï¼‰
        if not has_image and text:
            # è¿‡æ»¤æ‰ä¸€äº›æ— æ„ä¹‰çš„æ–‡å­—
            ignore_patterns = [
                '[å›¾ç‰‡:',
                'æ‰«æä»¶',
                'å¤å°ä»¶', 
                'æ­£æœ¬',
                'å‰¯æœ¬',
                'åŸä»¶',
                'é™„ä»¶'
            ]
            
            is_meaningful = True
            for pattern in ignore_patterns:
                if pattern in text:
                    is_meaningful = False
                    break
            
            # å¦‚æœæ–‡å­—é•¿åº¦å¤ªçŸ­ï¼Œä¹Ÿè®¤ä¸ºä¸æ˜¯æœ‰æ„ä¹‰çš„å†…å®¹
            if is_meaningful and len(text) > 5:
                meaningful_text.append(text)
    
    # å¦‚æœç« èŠ‚å†…åªæœ‰å›¾ç‰‡ï¼Œæ²¡æœ‰æœ‰æ„ä¹‰çš„æ–‡å­—
    return len(meaningful_text) == 0 and image_count > 0


def find_continuous_image_groups(doc, image_parts):
    """
    è¯†åˆ«æ–‡æ¡£ä¸­çš„è¿ç»­å›¾ç‰‡ç»„ï¼ˆåŒ…æ‹¬æ®µè½å’Œè¡¨æ ¼ä¸­çš„å›¾ç‰‡ï¼‰
    
    Args:
        doc: Wordæ–‡æ¡£å¯¹è±¡
        image_parts: å›¾ç‰‡éƒ¨ä»¶å­—å…¸
        
    Returns:
        list: è¿ç»­å›¾ç‰‡ç»„åˆ—è¡¨ï¼Œæ¯ç»„åŒ…å«å›¾ç‰‡ä¿¡æ¯å’Œç»„ID
    """
    all_images = []  # å­˜å‚¨æ‰€æœ‰å›¾ç‰‡çš„ä½ç½®ä¿¡æ¯
    
    # æ‰«ææ‰€æœ‰æ®µè½ï¼Œè®°å½•å›¾ç‰‡ä½ç½®
    for para_idx, paragraph in enumerate(doc.paragraphs):
        paragraph_images = []
        
        for run_idx, run in enumerate(paragraph.runs):
            if run._element.xml:
                for drawing in run._element.findall('.//w:drawing', 
                                                   {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}):
                    for blip in drawing.findall('.//a:blip', 
                                               {'a': 'http://schemas.openxmlformats.org/drawingml/2006/main'}):
                        embed_id = blip.get('{http://schemas.openxmlformats.org/officeDocument/2006/relationships}embed')
                        
                        if embed_id and embed_id in image_parts:
                            paragraph_images.append({
                                'para_idx': para_idx,
                                'run_idx': run_idx,
                                'embed_id': embed_id,
                                'paragraph': paragraph,
                                'location_type': 'paragraph',
                                'table_info': None
                            })
        
        all_images.extend(paragraph_images)
    
    # æ‰«ææ‰€æœ‰è¡¨æ ¼ï¼Œè®°å½•å›¾ç‰‡ä½ç½®
    table_para_offset = len(doc.paragraphs)  # è¡¨æ ¼å›¾ç‰‡çš„è™šæ‹Ÿæ®µè½ç´¢å¼•
    
    for table_idx, table in enumerate(doc.tables):
        for row_idx, row in enumerate(table.rows):
            for cell_idx, cell in enumerate(row.cells):
                for cell_para_idx, paragraph in enumerate(cell.paragraphs):
                    for run_idx, run in enumerate(paragraph.runs):
                        if run._element.xml:
                            for drawing in run._element.findall('.//w:drawing', 
                                                               {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}):
                                for blip in drawing.findall('.//a:blip', 
                                                           {'a': 'http://schemas.openxmlformats.org/drawingml/2006/main'}):
                                    embed_id = blip.get('{http://schemas.openxmlformats.org/officeDocument/2006/relationships}embed')
                                    
                                    if embed_id and embed_id in image_parts:
                                        # ä¸ºè¡¨æ ¼å›¾ç‰‡åˆ†é…è™šæ‹Ÿæ®µè½ç´¢å¼•
                                        virtual_para_idx = table_para_offset + table_idx * 100 + row_idx * 10 + cell_idx
                                        all_images.append({
                                            'para_idx': virtual_para_idx,
                                            'run_idx': run_idx,
                                            'embed_id': embed_id,
                                            'paragraph': paragraph,
                                            'location_type': 'table',
                                            'table_info': {
                                                'table_idx': table_idx,
                                                'row_idx': row_idx,
                                                'cell_idx': cell_idx,
                                                'cell_para_idx': cell_para_idx
                                            }
                                        })
    
    # æŒ‰ä½ç½®ç´¢å¼•æ’åº
    all_images.sort(key=lambda x: x['para_idx'])
    
    # è¯†åˆ«è¿ç»­å›¾ç‰‡ç»„
    image_groups = []
    current_group = []
    group_id = 0
    
    for i, img_info in enumerate(all_images):
        if not current_group:
            # å¼€å§‹æ–°ç»„
            current_group = [img_info]
        else:
            # åˆ¤æ–­æ˜¯å¦å±äºåŒä¸€è¿ç»­ç»„
            prev_img = current_group[-1]
            
            # å¯¹äºæ®µè½å›¾ç‰‡ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘
            if (img_info['location_type'] == 'paragraph' and 
                prev_img['location_type'] == 'paragraph'):
                
                para_gap = img_info['para_idx'] - prev_img['para_idx']
                is_continuous = True
                
                if para_gap > 0:
                    # æ£€æŸ¥ä¸­é—´æ®µè½æ˜¯å¦éƒ½æ˜¯ç©ºç™½æˆ–åªæœ‰å¾ˆå°‘æ–‡å­—
                    for check_para_idx in range(prev_img['para_idx'] + 1, img_info['para_idx']):
                        check_para = doc.paragraphs[check_para_idx]
                        if not is_empty_or_whitespace_paragraph(check_para):
                            if len(check_para.text.strip()) > 20:
                                is_continuous = False
                                break
                
                # å¦‚æœé—´éš”è¶…è¿‡3ä¸ªæ®µè½ï¼Œä¹Ÿè®¤ä¸ºä¸è¿ç»­
                if para_gap > 3:
                    is_continuous = False
                    
            # å¯¹äºè¡¨æ ¼å›¾ç‰‡ï¼ŒåŒä¸€è¡¨æ ¼åŒä¸€è¡Œçš„è¿ç»­å•å…ƒæ ¼è®¤ä¸ºæ˜¯è¿ç»­çš„
            elif (img_info['location_type'] == 'table' and 
                  prev_img['location_type'] == 'table'):
                
                curr_table = img_info['table_info']
                prev_table = prev_img['table_info']
                
                # åŒä¸€è¡¨æ ¼åŒä¸€è¡Œçš„ç›¸é‚»å•å…ƒæ ¼
                is_continuous = (curr_table['table_idx'] == prev_table['table_idx'] and
                               curr_table['row_idx'] == prev_table['row_idx'] and
                               abs(curr_table['cell_idx'] - prev_table['cell_idx']) <= 2)
                
            # è¡¨æ ¼å›¾ç‰‡å’Œæ®µè½å›¾ç‰‡ä¹‹é—´ä¸€èˆ¬ä¸è®¤ä¸ºè¿ç»­
            else:
                is_continuous = False
            
            if is_continuous:
                # å±äºå½“å‰ç»„
                current_group.append(img_info)
            else:
                # ç»“æŸå½“å‰ç»„ï¼Œå¼€å§‹æ–°ç»„
                if len(current_group) > 1:
                    # åªæœ‰å¤šäº1å¼ å›¾ç‰‡æ‰ç®—è¿ç»­ç»„
                    for img in current_group:
                        img['group_id'] = group_id
                        img['group_size'] = len(current_group)
                    group_id += 1
                else:
                    # å•å¼ å›¾ç‰‡ä¸ç®—ç»„
                    current_group[0]['group_id'] = None
                    current_group[0]['group_size'] = 1
                
                image_groups.extend(current_group)
                current_group = [img_info]
    
    # å¤„ç†æœ€åä¸€ç»„
    if current_group:
        if len(current_group) > 1:
            for img in current_group:
                img['group_id'] = group_id
                img['group_size'] = len(current_group)
        else:
            current_group[0]['group_id'] = None
            current_group[0]['group_size'] = 1
        image_groups.extend(current_group)
    
    return image_groups


def convert_doc_to_docx(doc_path):
    """å°†.docè½¬æ¢ä¸º.docx"""
    if not DOC_SUPPORT:
        raise Exception("ä¸æ”¯æŒ.docæ ¼å¼ï¼Œè¯·å…ˆè½¬æ¢ä¸º.docxæˆ–å®‰è£…pywin32")
    
    print(f"æ­£åœ¨è½¬æ¢ {doc_path} ...")
    word_app = win32com.client.Dispatch("Word.Application")
    word_app.Visible = False
    
    try:
        doc = word_app.Documents.Open(str(Path(doc_path).absolute()))
        docx_path = str(Path(doc_path).with_suffix('.docx'))
        doc.SaveAs2(docx_path, FileFormat=16)
        doc.Close()
        word_app.Quit()
        return docx_path
    except Exception as e:
        word_app.Quit()
        raise e


def extract_and_separate(input_file):
    """
    ä¸»è¦åŠŸèƒ½ï¼šæå–å›¾ç‰‡å¹¶åˆ†ç¦»
    
    Args:
        input_file: è¾“å…¥çš„Wordæ–‡æ¡£è·¯å¾„
        
    Returns:
        tuple: (åˆ†ç¦»åçš„æ–‡æ¡£è·¯å¾„, å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„, å›¾ç‰‡æ•°é‡)
    """
    input_path = Path(input_file)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not input_path.exists():
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
    
    # å¤„ç†.docæ–‡ä»¶
    if input_path.suffix.lower() == '.doc':
        input_file = convert_doc_to_docx(input_file)
        input_path = Path(input_file)
    elif input_path.suffix.lower() != '.docx':
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {input_path.suffix}")
    
    # åˆ›å»ºè¾“å‡ºè·¯å¾„
    base_name = input_path.stem
    output_doc = input_path.parent / f"{base_name}_åˆ†ç¦»å.docx"
    output_images = input_path.parent / f"{base_name}_å›¾ç‰‡"
    output_images.mkdir(exist_ok=True)
    
    # åŠ è½½æ–‡æ¡£
    doc = Document(input_file)
    
    # åˆå§‹åŒ–AIå®¢æˆ·ç«¯
    ai_client = init_ai_client()
    
    # è·å–æ‰€æœ‰å›¾ç‰‡
    image_parts = {}
    for rel in doc.part.rels.values():
        if "image" in rel.target_ref:
            image_parts[rel.rId] = rel.target_part

    # åˆå§‹åŒ–AIå®¢æˆ·ç«¯
    ai_client = init_ai_client()
    
    # è¯†åˆ«è¿ç»­å›¾ç‰‡ç»„
    image_groups = find_continuous_image_groups(doc, image_parts)
    
    image_count = 0
    processed_images = set()  # è®°å½•å·²å¤„ç†çš„å›¾ç‰‡ï¼Œé¿å…é‡å¤å¤„ç†
    group_base_names = {}  # ç¼“å­˜æ¯ä¸ªç»„çš„åŸºç¡€åç§°

    # æŒ‰è¿ç»­ç»„å¤„ç†å›¾ç‰‡
    for img_info in image_groups:
        if img_info['embed_id'] in processed_images:
            continue
            
        image_count += 1
        processed_images.add(img_info['embed_id'])
        
        # æ”¶é›†ä¸Šä¸‹æ–‡ - æ”¹è¿›ç‰ˆæœ¬
        context_text = ""
        
        # é¦–å…ˆç¡®å®šå›¾ç‰‡åœ¨ç»„ä¸­çš„ä½ç½®
        if img_info['group_id'] is not None:
            # æ˜¯è¿ç»­ç»„çš„ä¸€éƒ¨åˆ†ï¼Œè®¡ç®—åœ¨ç»„ä¸­çš„ä½ç½®
            group_images = [img for img in image_groups if img['group_id'] == img_info['group_id']]
            group_images.sort(key=lambda x: x['para_idx'])
            img_index_in_group = group_images.index(img_info) + 1
            total_in_group = len(group_images)
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»ä¸ºè¿™ä¸ªç»„ç”Ÿæˆäº†åŸºç¡€åç§°
            if img_info['group_id'] in group_base_names:
                # ä½¿ç”¨å·²ç”Ÿæˆçš„åŸºç¡€åç§°
                base_name = group_base_names[img_info['group_id']]
                naming_method = "è¿ç»­ç»„å…±äº«å‘½å"
            else:
                # è¿™æ˜¯ç»„ä¸­çš„ç¬¬ä¸€å¼ å›¾ç‰‡ï¼Œéœ€è¦ç”ŸæˆåŸºç¡€åç§°
                base_name = None
                naming_method = ""
        else:
            # å•ç‹¬å›¾ç‰‡
            img_index_in_group = 1
            total_in_group = 1
            base_name = None
            naming_method = ""
        
        # é¦–å…ˆæ”¶é›†ä¸Šä¸‹æ–‡ä¿¡æ¯ç”¨äºAIå‘½å
        # 1. è·å–æ–‡æ¡£æ ‡é¢˜
        doc_title = ""
        if doc.paragraphs:
            first_para = doc.paragraphs[0].text.strip()
            if first_para:
                doc_title = first_para[:50]  # å–å‰50ä¸ªå­—ç¬¦ä½œä¸ºæ–‡æ¡£æ ‡é¢˜
        
        # 2. ä¸ºæ¯å¼ å›¾ç‰‡å¯»æ‰¾æœ€è¿‘çš„5æ®µæœ‰æ•ˆä¸Šä¸‹æ–‡æ–‡å­—
        nearest_contexts = find_nearest_context_paragraphs(doc, img_info['para_idx'])
        
        # 3. ç»„åˆä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆä¼˜å…ˆä½¿ç”¨æœ€è¿‘çš„5æ®µä¸Šä¸‹æ–‡ï¼‰
        context_parts = []
        
        # æ·»åŠ æ–‡æ¡£æ ‡é¢˜
        if doc_title:
            context_parts.append(f"æ–‡æ¡£æ ‡é¢˜: {doc_title}")
        
        # æ·»åŠ æœ€è¿‘çš„5æ®µä¸Šä¸‹æ–‡ï¼ˆè¿™æ˜¯æœ€é‡è¦çš„ä¿¡æ¯ï¼‰
        if nearest_contexts:
            context_parts.append("æœ€æœ‰æ•ˆä¸Šä¸‹æ–‡:")
            for i, ctx in enumerate(nearest_contexts, 1):
                # é™åˆ¶æ¯æ®µä¸Šä¸‹æ–‡çš„é•¿åº¦ä»¥é¿å…æç¤ºè¯è¿‡é•¿
                ctx_text = ctx['text'][:100] if len(ctx['text']) > 100 else ctx['text']
                value_info = f"è¯„åˆ†{ctx['value_score']}"
                if ctx['has_key_info']:
                    value_info += ",å…³é”®ä¿¡æ¯"
                if ctx['is_heading']:
                    value_info += ",æ ‡é¢˜"
                context_parts.append(f"  {i}. æ®µè½{ctx['para_idx']}(è·ç¦»{ctx['distance']},{value_info}): {ctx_text}")
        
        # æ·»åŠ é¢å¤–çš„è§’è‰²å…³é”®è¯ï¼ˆå¦‚æœç©ºé—´å…è®¸ï¼‰
        role_keywords = ["æˆæƒä»£ç†äºº", "æˆæƒå§”æ‰˜äºº", "æ³•å®šä»£è¡¨äºº", "å•ä½è´Ÿè´£äºº", "è‡ªç„¶äºº", "èº«ä»½è¯", "è¥ä¸šæ‰§ç…§", "æ‰§ç…§"]
        role_context = []
        
        for para_idx, para in enumerate(doc.paragraphs):
            para_text = para.text.strip()
            if para_text:
                for keyword in role_keywords:
                    if keyword in para_text:
                        role_context.append(f"{keyword}: {para_text[:60]}")  # ç¼©çŸ­é•¿åº¦
                        break
                        
        # åªæ·»åŠ æœ€ç›¸å…³çš„è§’è‰²ä¸Šä¸‹æ–‡ï¼ˆæœ€å¤š2ä¸ªï¼‰
        if role_context:
            context_parts.append("è§’è‰²ä¿¡æ¯:")
            for role_ctx in role_context[:2]:
                context_parts.append(f"  - {role_ctx}")
            
        context_text = "\n".join(context_parts)
        
        # ä¼˜å…ˆAIå‘½åï¼ŒAIä¸å¯ç”¨æˆ–AIå‘½åä¸åˆç†æ—¶fallbackä¸ºæ ‡é¢˜å
        ai_available = ai_client is not None
        if not base_name and ai_available and context_text.strip():
            try:
                if os.getenv('AI_NAME_LOG'):
                    print(f"ğŸ¤– å‡†å¤‡è°ƒç”¨AIå‘½å...")
                    print(f"   å›¾ç‰‡æ®µè½: {img_info['para_idx']}")
                    print(f"   ç»„ID: {img_info['group_id']}")
                    print(f"   ä¸Šä¸‹æ–‡é•¿åº¦: {len(context_text)} å­—ç¬¦")
                    print(f"   ä¸Šä¸‹æ–‡é¢„è§ˆ: {context_text[:200]}...")
                ai_name = generate_image_name(
                    ai_client, 
                    context_text.strip(),
                    img_index_in_group,
                    total_in_group
                )
                if os.getenv('AI_NAME_LOG'):
                    print(f"ğŸ¯ AIè¿”å›ç»“æœ: '{ai_name}'")
                # ä»AIå‘½åä¸­æå–åŸºç¡€åç§°
                base_name = re.sub(r'\d+$', '', ai_name)  # ç§»é™¤æœ«å°¾æ•°å­—
                if not base_name:
                    base_name = ai_name
                # æ ¡éªŒAIå‘½åæ˜¯å¦åˆç†
                if _is_ai_result_valid(base_name, context_text):
                    naming_method = "AIæ™ºèƒ½å‘½å"
                    if img_info['group_id'] is not None:
                        group_base_names[img_info['group_id']] = base_name
                        if os.getenv('AI_NAME_LOG'):
                            print(f"ğŸ’¾ ä¸ºç»„ {img_info['group_id']} ç¼“å­˜åŸºç¡€åç§°: '{base_name}'")
                else:
                    if os.getenv('AI_NAME_LOG'):
                        print(f"âš ï¸ AIå‘½åä¸åˆç†: '{base_name}'ï¼Œä½¿ç”¨fallback")
                    base_name = None
            except Exception as e:
                if os.getenv('AI_NAME_LOG'):
                    print(f"âš ï¸ AIå‘½åå¤±è´¥: {e}")
                ai_available = False
                base_name = None
        
        # å¦‚æœAIå‘½åå¤±è´¥æˆ–ä¸å¯ç”¨ï¼Œç›´æ¥é€‰å–å›¾ç‰‡æ‰€å±çš„æ ‡é¢˜å
        if not base_name and nearest_contexts:
            # ä¼˜å…ˆä½¿ç”¨æœ€è¿‘çš„æ ‡é¢˜
            for context in nearest_contexts:
                if context.get('is_heading') and context['text']:
                    title = context['text'].strip()
                    clean_title = re.sub(r'[^\w\u4e00-\u9fff]', '', title)
                    if 2 <= len(clean_title) <= 15:
                        base_name = clean_title
                        naming_method = "æ ‡é¢˜å‘½å"
                        break
            # å¦‚æœæ²¡æœ‰åˆé€‚çš„æ ‡é¢˜ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªé«˜ä»·å€¼ä¸Šä¸‹æ–‡

            if not base_name:
                first_context = nearest_contexts[0]
                if first_context.get('has_key_info') and first_context['text']:
                    text = first_context['text'].strip()
                    clean_text = re.sub(r'[^\w\u4e00-\u9fff]', '', text)
                    if 2 <= len(clean_text) <= 15:
                        base_name = clean_text
                        naming_method = "ä¸Šä¸‹æ–‡å‘½å"


# --- ä¿è¯é¡¶æ ¼å®šä¹‰ ---
def _is_ai_result_valid(ai_name, context_text):
    """
    æ ¡éªŒAIè¿”å›çš„å‘½åæ˜¯å¦åˆç†
    """
    if not ai_name or len(ai_name.strip()) == 0:
        return False
    ai_name = ai_name.strip()
    # å¦‚æœAIå‘½åä¸ºè¥ä¸šæ‰§ç…§ï¼Œä½†ä¸Šä¸‹æ–‡æ²¡æœ‰è¥ä¸šæ‰§ç…§ï¼Œåˆ™åˆ¤ä¸ºä¸åˆç†
    if 'è¥ä¸šæ‰§ç…§' in ai_name and 'è¥ä¸šæ‰§ç…§' not in context_text:
        return False
    # å…¶å®ƒç®€å•è§„åˆ™ï¼šå‘½åé•¿åº¦åˆç†ä¸”ä¸æ˜¯çº¯æ•°å­—
    if 2 <= len(ai_name) <= 12 and not ai_name.isdigit():
        return True
    return False

    # å¦‚æœæœ€è¿‘ä¸Šä¸‹æ–‡ä¹Ÿæ²¡æœ‰ï¼Œæ£€æŸ¥ç« èŠ‚æ ‡é¢˜å‘½å
if not base_name:
    headings = collect_headings(doc)
    section_info, section_start, section_end = find_section_for_image(headings, img_info['para_idx'], len(doc.paragraphs))
    if section_info and check_section_content(doc, section_start, section_end, img_info['para_idx']):
        section_title = section_info['text']
        base_name = re.sub(r'[^\w\u4e00-\u9fff]', '', section_title)
        if len(base_name) > 12:
            base_name = base_name[:12]
        naming_method = "ç« èŠ‚æ ‡é¢˜å‘½å"

# æœ€åçš„å…œåº•æ–¹æ¡ˆ
if not base_name:
    base_name = "å›¾ç‰‡"
    naming_method = "é»˜è®¤å‘½å"
        
        # è°ƒè¯•æ—¥å¿—
        if os.getenv('AI_NAME_LOG'):
            print(f"ğŸ¤– ä½¿ç”¨{naming_method}: '{base_name}'")
            if nearest_contexts:
                print(f"ğŸ” æ‰¾åˆ°çš„5æ®µæœ€æœ‰æ•ˆä¸Šä¸‹æ–‡:")
                for i, ctx in enumerate(nearest_contexts, 1):
                    ctx_type_info = []
                    if ctx['has_key_info']:
                        ctx_type_info.append("å…³é”®ä¿¡æ¯")
                    if ctx['is_heading']:
                        ctx_type_info.append("æ ‡é¢˜")
                    ctx_type = ",".join(ctx_type_info) if ctx_type_info else "æ™®é€š"
                    print(f"   {i}. æ®µè½{ctx['para_idx']}(è·ç¦»{ctx['distance']},è¯„åˆ†{ctx['value_score']},{ctx_type}): {ctx['text'][:50]}...")
        
        # ä½¿ç”¨æ–°çš„å‘½åæ ¼å¼ï¼šå›¾ç‰‡é¡ºåº+å›¾ç‰‡åç§°+ï¼ˆè¿ç»­å›¾ç‰‡åºå·ï¼‰
        if total_in_group > 1:
            final_name = f"{image_count}{base_name}ï¼ˆ{img_index_in_group}ï¼‰"
        else:
            final_name = f"{image_count}{base_name}"
        
        # è°ƒè¯•æ—¥å¿—
        if os.getenv('AI_NAME_LOG'):
            print(f"ğŸ·ï¸  æœ€ç»ˆå‘½å: '{final_name}' (åŸºç¡€å: {base_name}, å›¾ç‰‡åºå·: {image_count}, ç»„å†…åºå·: {img_index_in_group}/{total_in_group})")
        
        # ä¿å­˜å›¾ç‰‡
        image_filename = save_image(
            image_parts[img_info['embed_id']], 
            output_images, 
            image_count,
            custom_name=final_name
        )
        
        # åœ¨æ–‡æ¡£ä¸­æ›¿æ¢å›¾ç‰‡ä¸ºæ–‡æœ¬æ ‡è®°
        paragraph = img_info['paragraph']
        run_idx = img_info['run_idx']
        
        # ç§»é™¤åŒ…å«å›¾ç‰‡çš„run
        if run_idx < len(paragraph.runs):
            paragraph._element.remove(paragraph.runs[run_idx]._element)
        
        # æ·»åŠ æ›¿æ¢æ–‡æœ¬
        replacement_text = f"[å›¾ç‰‡: {image_filename}]"
        if paragraph.runs:
            # å¦‚æœæ®µè½è¿˜æœ‰å…¶ä»–runsï¼Œæ·»åŠ åˆ°ç¬¬ä¸€ä¸ªrun
            if paragraph.runs[0].text:
                paragraph.runs[0].text = replacement_text + " " + paragraph.runs[0].text
            else:
                paragraph.runs[0].text = replacement_text
        else:
            # å¦‚æœæ®µè½æ²¡æœ‰runsï¼Œåˆ›å»ºæ–°çš„run
            paragraph.add_run(replacement_text)

    # ä¿å­˜åˆ†ç¦»åçš„æ–‡æ¡£
    doc.save(str(output_doc))

    return str(output_doc), str(output_images), image_count


def save_image(image_part, output_dir, counter, custom_name=None):
    """ä¿å­˜å›¾ç‰‡æ–‡ä»¶"""
    image_data = image_part.blob
    
    # æ£€æµ‹å›¾ç‰‡æ ¼å¼
    try:
        img = Image.open(BytesIO(image_data))
        format_name = img.format.lower() if img.format else 'png'
        if format_name == 'jpeg':
            format_name = 'jpg'
    except:
        format_name = 'png'
    
    # ç¡®å®šæ–‡ä»¶å
    if custom_name:
        # ä½¿ç”¨è‡ªå®šä¹‰åç§°
        filename = f"{custom_name}.{format_name}"
    else:
        # ä½¿ç”¨é»˜è®¤å‘½å
        filename = f"å›¾ç‰‡_{counter:03d}.{format_name}"
    
    # å¤„ç†æ–‡ä»¶åå†²çª
    filepath = output_dir / filename
    duplicate_counter = 1
    while filepath.exists():
        if custom_name:
            filename = f"{custom_name}_{duplicate_counter}.{format_name}"
        else:
            filename = f"å›¾ç‰‡_{counter:03d}_{duplicate_counter}.{format_name}"
        filepath = output_dir / filename
        duplicate_counter += 1
    
    # ä¿å­˜å›¾ç‰‡
    with open(filepath, 'wb') as f:
        f.write(image_data)
    
    return filename


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 50)
    print("Wordæ–‡æ¡£å›¾ç‰‡åˆ†ç¦»å·¥å…·")
    print("=" * 50)
    
    # æ£€æŸ¥å¸®åŠ©
    if len(sys.argv) > 1 and sys.argv[1] in ['-h', '--help', 'help']:
        print("ä½¿ç”¨æ–¹æ³•:")
        print(f"  python {Path(__file__).name} <Wordæ–‡æ¡£è·¯å¾„>")
        print("\nç¤ºä¾‹:")
        print(f"  python {Path(__file__).name} document.docx")
        print(f"  python {Path(__file__).name} 'åŒ…å«ç©ºæ ¼çš„æ–‡æ¡£.docx'")
        print("\nè¾“å‡º:")
        print("  - æ–‡æ¡£å_åˆ†ç¦»å.docx (å›¾æ–‡åˆ†ç¦»åçš„æ–‡æ¡£)")
        print("  - æ–‡æ¡£å_å›¾ç‰‡/ (åŒ…å«æ‰€æœ‰å›¾ç‰‡çš„æ–‡ä»¶å¤¹)")
        return
    
    # è·å–è¾“å…¥æ–‡ä»¶
    if len(sys.argv) > 1:
        input_file = sys.argv[1]
    else:
        input_file = input("è¯·è¾“å…¥Wordæ–‡æ¡£è·¯å¾„: ").strip('"')
    
    if not input_file:
        print("é”™è¯¯: æœªæŒ‡å®šè¾“å…¥æ–‡ä»¶")
        print("ä½¿ç”¨ --help æŸ¥çœ‹å¸®åŠ©")
        return
    
    try:
        # æ‰§è¡Œåˆ†ç¦»
        print(f"æ­£åœ¨å¤„ç†: {Path(input_file).name}")
        output_doc, output_images, image_count = extract_and_separate(input_file)
        # æ˜¾ç¤ºç»“æœ
        print("\nå¤„ç†å®Œæˆ!")
        print(f"åˆ†ç¦»åæ–‡æ¡£: {Path(output_doc).name}")
        print(f"å›¾ç‰‡æ–‡ä»¶å¤¹: {Path(output_images).name}")
        print(f"æå–å›¾ç‰‡æ•°é‡: {image_count}")
        if image_count == 0:
            print("æ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡")
    except Exception as e:
        print(f"å¤„ç†å¤±è´¥: {e}")


if __name__ == "__main__":
    main()